# 【面筋】百面百搭 群讨论收集

本项目是作者们根据个人面试和经验总结出的自然语言处理(NLP)、推荐系统(RES) 面试准备的学习笔记与资料，该资料目前包含 自然语言处理和推荐各领域的 面试题积累。

- [【面筋】百面百搭 群讨论收集](#面筋百面百搭-群讨论收集)
  - [问题十：LSTM为什么能解决梯度消失和梯度爆炸(自然语言处理)](#问题十lstm为什么能解决梯度消失和梯度爆炸自然语言处理)
  - [问题九：xgb怎么处理缺省值](#问题九xgb怎么处理缺省值)
  - [问题八：怎么处理未登录词问题](#问题八怎么处理未登录词问题)
  - [问题七：对于生僻词，skip-gram和cbow哪个效果更好？原因是什么？(自然语言处理)](#问题七对于生僻词skip-gram和cbow哪个效果更好原因是什么自然语言处理)
  - [问题六：超过1000个字符长度的文本分类问题如何解决？(自然语言处理)](#问题六超过1000个字符长度的文本分类问题如何解决自然语言处理)
  - [问题一：召回模型中，模型评价指标怎么设计？（推荐）](#问题一召回模型中模型评价指标怎么设计推荐)
  - [问题二：w&d模型的特征，哪些放到wide侧，哪些放到deep侧？ w&d模型在线上，如何做到根据实时数据更新模型（推荐）](#问题二wd模型的特征哪些放到wide侧哪些放到deep侧-wd模型在线上如何做到根据实时数据更新模型推荐)

## 问题十：LSTM为什么能解决梯度消失和梯度爆炸(自然语言处理)

```s
dz-Summer:
大佬们，新一期 TopicShare 的 Topic 分享 填报来了，大家可以填一下感兴趣的 Topic

Topic8 TopicShape Topic收集:https://shimo.im/sheets/y98CHkv6KHvkwrQ3/R6vm5/ 

【#每日一题#】

LSTM为什么能解决梯度消失和梯度爆炸(自然语言处理)

面试题收集问卷：https://shimo.im/forms/Gd3jQWDpgyd3xcJ3/fill
【面筋】百面百搭：https://github.com/km1994/Interview-Notes

dz-刘鸿儒:
可以解决梯度爆炸，解决不了梯度消失?

dz-刘鸿儒:
因为有门，可以限制输出的范围

dz-Summer:
欢迎大佬们积极讨论，题目中隐藏的一些问题，直接怼

dz-稻米:
他不能解决，只能缓解，因为引入了门控机制，由rnn的累乘改为了累和，反向传播梯度消失的影响较大的缓解，细胞状态c也可以较好的存储长序列的语义信息

Dw-成员-李祖贤-萌弟:
至今没能解决

dz-刘鸿儒:
对，缓解

```

## 问题九：xgb怎么处理缺省值

```s
dz-琼花梦落:
还问xgb怎么处理缺省值

dz-琼花梦落:
咋处理啊

冯。:
训练时候 选择分裂后增益最大的那个方向（左分支或是右分支）

just do it!:
有大佬愿意参与 群讨论内容 整理么？

冯。:
预测时候 出现新缺失 默认向右？

dz-夜伦:
论文好像是左

冯。:
我记得是右

dz-夜伦:
这个应该是训练的时候没有碰到缺失值，而预测的时候碰到缺失值

dz-夜伦:
如果在训练当中碰到  缺失值不参与损失下降吧 好像做都会分配到左右节点

dz-夜伦:
看哪个增益大 作为默认分裂方向

dz-琼花梦落:
我没用过，都没答上来，上面回答的两位应该更懂

冯。:
默认向左 我错了

冯。:
https://zhuanlan.zhihu.com/p/382231757

```

## 问题八：怎么处理未登录词问题

```s
OPPO-文笔超好的男同学:
问题可以考虑升级泛化一下，怎么处理未登录词问题，这样更加开放性

just do it!:
可以可以

dz-稻米:
太狠了，会问这么深嘛

dz-琼花梦落:
今天下午

dz-稻米:
好像一般都问到如何将层次softmax和负采样融入进模型训练的，差不多就给过了

just do it!:
尽可能找还原语义

1.原始词有没有
2.全小写有没有
3.全大写有没有
4.首字母大写有没有
5.三种次干化有没有
6.长得最像的几种编辑方法有没有

OPPO-文笔超好的男同学:
问这种开放问题，基本都能答，然后可以跟着回答继续问下去

dz-琼花梦落:
也问了word2vec里面分层softmax和负采样

just do it!:
https://www.zhihu.com/question/308543084/answer/576517555

just do it!:
挨家挨户 找亲家
```

## 问题七：对于生僻词，skip-gram和cbow哪个效果更好？原因是什么？(自然语言处理)

```python
【每日一题】
对于生僻词，skip-gram和cbow哪个效果更好？原因是什么？
(自然语言处理)

面试题收集问卷：https://shimo.im/forms/Gd3jQWDpgyd3xcJ3/fill
【面筋】百面百搭：https://github.com/km1994/Interview-Notes

文笔超好的男同学:
生僻字频次太低，大概率都是会因为这个而没法训练

文笔超好的男同学:
我感觉就是统计词频，在正类和在负类里频次差别巨大的可以考虑下

Stefan:
skip gram对低频高频都差不多

Stefan:
cbow 更倾向高频

Stefan:
训练速度上看。cbow快一些，但获取的信息少一些

刘乙己??:
skip_gram:一个学生K个老师

刘乙己??:
cbow:一个老师要教K个学生

just do it!:
但是，我们在训练词向量的时候，其实会把低频词（出现1-2词）的过滤掉的，所以很多时候不用考虑吧，而且对于一个出现词数极低的，对于模型的价值也不大

just do it!:
我是觉得需要结合业务场景出发，比如如果是医疗领域，会出现很多 生僻词（eg：骨骺）等，但是如果语料中出现次数多的话，其实是可以学到的。因为从我们主观觉得“骨骺”是生僻词，但是从模型角度讲，骨骺 就不算是生僻词了，而是高频词
```

## 问题六：超过1000个字符长度的文本分类问题如何解决？(自然语言处理)

```s
大佬们，新一期 TopicShare 的 Topic 分享 填报来了，大家可以填一下感兴趣的 Topic

Topic8 TopicShape Topic收集:https://shimo.im/sheets/y98CHkv6KHvkwrQ3/R6vm5/ 

【每日一题】
超过1000个字符长度的文本分类问题如何解决？
(自然语言处理)

面试题收集问卷：https://shimo.im/forms/Gd3jQWDpgyd3xcJ3/fill
【面筋】百面百搭：https://github.com/km1994/Interview-Notes

文笔超好的男同学:
还是看场景看问题吧，点太多了

文笔超好的男同学:
长串dna测序用nlp都能解，长文本本身不是难题，难得是问题定义和数据

我的女票美如画:
1. 抽取 2. 截断 3. 输入层处理压缩维度 4. 对输入没限制的预训练模型 我只知道这几个方案

文笔超好的男同学:
举个例子吧，情感分析类，长文本本身情感可能复杂，那处理起来甚至标签都有问题

王泽军:
假设现在有个语料库，每个样本是1000多字的文档，标签是对应的文档类别。怎么做分类效果比较好？

文笔超好的男同学:
经验上长文本用截断，然后用cnn-lstm做基线试试

王翔:
长文本确实不是问题，很多变形都已经cover住长度了，关键也是上线压力和真正的全文信息吧

文笔超好的男同学:
本身难度就是数据和标签本身的关系

文笔超好的男同学:
因为文本太长信息太多，标签很多时候拿不准，别说模型，人都不容易

王翔:
是的，其实看做啥了，太多无用信息了

大雨:
需要先做抽取再分类吗？

王泽军:
如果长文本有摘要内容的话，可以拿摘要出来做分类

文笔超好的男同学:
玄学的说，tfidf加机器学习效果说不定不比深度学习差

:
太多无用信息..

:
对 就规则吧

王泽军:
用 tfidf + libsvm 做基线试试

大雨:
有没有先做摘要再分类的做法?

文笔超好的男同学:
中文长文本分类数据集

文笔超好的男同学:
那就要看摘要准不准了

王泽军:
可以先用简单的方法提取一些关键句，再用深度学习的方法分类，说不定效果不错

文笔超好的男同学:
准确率肯定要打个折的

王翔:
摘要本来就做的一般，又加一层损失的感觉，其实滑动截断或者层次都还好，我感觉除非强相关，反正其实一般分类截断够用了

大雨:
是不是得把反应段落主旨的句子选出来

大雨:
应该是以句子为基本单位来选?

王翔:
这个看需求吧

大雨:
所以怎么选主旨句有好办法吗？

王翔:
@大雨?有标签，做段落排序而已

大雨:
具体咋排呢？如果标注了一段话的主旨句的话

王翔:
最简单就是看打分，打分高就是

大雨:
是谁对谁打分？

王翔:
对句子打分，预测时主旨句的概率，最简单不就是做二分类吗

大雨:
输入是整个段落吗？整个段落本身都已经太长了‘

王翔:
怎么做，看具体做的思路，句子+上下文

大雨:
是指每个句子只用上下相邻的几个句子做二分类？而不需要整段

just do it!:
1. 规则：
1.1 截取；
1.2 划窗

2. 模型
2.1 Transformer-XL
2.2 XLNet

3. trick
3.1 做 文本摘要，提取主干，再用主干做分类

大雨:
主要是怎样提取主干

LLLuna:
用预训练模型对句子进行语义表示，如果是对句子所有token取最后一层编码的平均，是不是要mask掉padding的那些token的表示？
```

## [问题一：召回模型中，模型评价指标怎么设计？（推荐）](README.md#问题一召回模型中模型评价指标怎么设计推荐)

## [问题二：w&d模型的特征，哪些放到wide侧，哪些放到deep侧？ w&d模型在线上，如何做到根据实时数据更新模型（推荐）](README.md#问题二wd模型的特征哪些放到wide侧哪些放到deep侧-wd模型在线上如何做到根据实时数据更新模型推荐)

> 精彩讨论
```s
刘乙己??:
w&d模型在线上，如何做到根据实时数据更新模型
(推荐)?
模型服务器每次收到（app召回集合+当前用户特征），模型服务器返回每个app的score。
score就是对于wide & deep模型的一次 forward pass。
为了缩短响应时间10ms以内，并行预测，例如可以将分布式多线程的计算不同批次app的得分。

拒绝焦虑李某人:
https://zhuanlan.zhihu.com/p/75597761

拒绝焦虑李某人:
我看教程说，w&d里边wide的部分的优化器是用的FTRL做在线学习，利用少量样本对参数更新做类似截断的方法，满足某个条件跟新，不满足不更新，deep侧用的adagrad用全量数据做训练，我贴这个问题是想问，线上的w&d，wide可以做在线学习，那deep侧就不用了么

拒绝焦虑李某人:
王吉吉老师的一个学习笔记里写，全量数据用SGD adagrad等传统优化器来更新参数，增量数据做在线学习的时候用FTRL等，但是w&d里只有wide侧的优化器是FTRL啊

大雨:
sgd adagrad也可以增量更新啊

拒绝焦虑李某人:
增量数据带来的梯度有可能会是不正确的方向

拒绝焦虑李某人:
就是类似脏数据的意思

拒绝焦虑李某人:
如果用sgd或者adagrad没传一次增量数据就更新，模型不会跑偏么

大雨:
实时性和全局准确性是矛盾的

大雨:
可以2个版本我猜，和老版本pk一下，新的把老的赢了才用新的

拒绝焦虑李某人:
我看了下wide侧用FTRL+L1正则化是为了让模型参数更加稀疏，更在线学习没关系至于wide&deep怎么做在线学习参数实时更新，这个问题好像挺少见

刘乙己??:
我认为这个更新还是根据wide&deep这个的数据类型进行判断的叭
wide是原始特征，一些固定的数据，他不是经常变动；deep是数值特征，类别特征，是要求实时性

刘乙己??:
如果一些固定数据的变动的话，还是要进行更新的叭

刘乙己??:
更新频率的不同，导致相对来说，wide是不进行更新的

大雨:
所以要实时都实时要不实时都不实时，没有分开处理一说

拒绝焦虑李某人:
是的

大雨:
我觉得不是这个，但我说不来是哪个

_Rulcy:
我也看了一下，解释还是和业务有点关系的。论文里wide侧数据维度还是挺高的，所以需要控制Wide侧稀疏程度保证在线模型实时性。这里在线学习和模型稀疏还是紧密相关的，是因为在线学习引入的单个样本更新更可能影响到模型全局的权重，使模型丧失稀疏性，所以采用了FTRL进行更新。至于为什么只考虑Wide不考虑Deep侧实时性，是因为模型输入时Deep通常输入Embedding，相对Wide的LR模型输入One-hot那种，维度低了很多，所以Deep侧更看重精度，而对于实时性没有过多考虑。

_Rulcy:

_Rulcy:
原答案在这里，根本原因还是稀疏性和在线模型复杂度是紧密相关的

大雨:
为什么deep侧不考虑稀疏性，也会有很多id特征啊

_Rulcy:
原因是模型上线后，在wide输入是稀疏向量，而deep输入是embedding向量。wide侧稀疏的话，线上模型对wide做的压缩提升是很大的。

_Rulcy:
单独提出deep侧来看，如果使特征输入层到下一隐层稀疏的话，丧失了本来的目的，就是本来Deep就是考虑用几近全量的特征以提高模型泛化能力的，没必要再做一次特征筛选。如果使模型整体稀疏的话，同样可以在设置模型参数时就让Deep侧更轻量，不需要设置一个大的网络再通过稀疏化来缩减模型。

大雨:
可以让deep侧的时候sparse层更稀疏啊

_Rulcy:
可以但是没必要啊，拖慢网络速度的主要就是输入到embedding，Deep侧模型上线后直接采用embedding向量作为输入了，网络已经优化到一定程度了

_Rulcy:
就举例子来说，经过稀疏后，wide侧效率从100ms->10ms。经过同样的稀疏，deep侧效率可以从10ms->1ms，优化程度还是有很大区别的

大雨:
增加稀疏性还能提速?

_Rulcy:
是指线下训练增加稀疏性，线上模型就可以通过压缩提速了

大雨:
我的理解，压缩是通过降低网络开销提速的?

_Rulcy:
是这样的，简单来说一个LR模型，通过稀疏使它一半的权重为0，那么模型上限时就可以把这些权重去掉，提高效率

大雨:
所以大头还是省了网络通信开销，cpu/gpu计算耗时基本不变

_Rulcy:
emmm倒不是这个意思。这里理解两点就很好懂了：1.线下训练的模型和线上服务的模型有很大的差别，所有最终的目的都是为线上模型服务的。2.线下模型中，Deep侧训练开销更大。但是模型上线后，Wide的可能反而压力是更大的。

_Rulcy:
提高稀疏性对于线下的模型肯定是计算耗时基本不变的，但是对于线上模型一定是有提升的。

大雨:
wide端的压力更多来自存储?

_Rulcy:
wide侧的压力来自于LR没有embedding层

大雨:
忽略存储因素，忽略网络通信因素，稀疏与否并不影响，我觉得

大雨:
没有embedding层照样可以把向量变成标量，存redis

_Rulcy:
类别特征存储并不是难事啊，只是在预测时，没有Emb层的输入是稀疏的，有Emb层输入是稠密的

_Rulcy:
这种稀疏向量才是影响网络计算最大的问题

大雨:
没有问题embedding层的输入也可以是id啊，进行查表

大雨:
只不过查的不是向量，是权重，是标量

_Rulcy:
是可以的，但是主要还是one-hot嘛。类别型特征如果转化为标量，效果肯定是比不上one-hot的

_Rulcy:
如果你让LR的输入层都转为标量型，那肯定没有上面的这些顾虑了。但是为了效果，LR层的输入都是One-Hot的

_Rulcy:
类别特征转化为标量最大的问题还是，标量之间的差很难解释其意义，但又是一个重要性质

_Rulcy:
所以可以采用标量存储，但是预测时不推荐

拒绝焦虑李某人:
我觉得模型计算时间跟特征维度有关，跟网络深度也有关，跟有网络是否稀疏无关

大雨:
我不是直接转成标量，而是将id和对应的权重k,v存储

_Rulcy:
你忽视的点是，线下模型是稀疏的，线上模型会剔除稀疏权重。

拒绝焦虑李某人:
也就是参数为0的直接不算了么

_Rulcy:
对的

_Rulcy:
网络压缩也是一个热门研究方向的

大雨:
跟实现有关

大雨:
我觉得网络通信传输是大头

拒绝焦虑李某人:
deep侧不在线学习的原因是它没法稀疏化是么

大雨:
wide侧的职责我觉得是快速地捕捉变化

_Rulcy:
是稀疏化取得的提升不是太大

_Rulcy:
下班了还是给大伙推荐点好文章吧，一个很形象的将梯度下降方法的文章。http://www.360doc.com/content/20/0611/01/32196507_917690415.shtml

_Rulcy:
感觉理解更深了哈哈哈

大雨:
deep侧因为比较深，学得慢，所以要厚积薄发，举一反三

大雨:
dnn的稀疏化本来就很难
```

