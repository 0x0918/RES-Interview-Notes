# 【面筋】百面百搭 群讨论收集

本项目是作者们根据个人面试和经验总结出的自然语言处理(NLP)、推荐系统(RES) 面试准备的学习笔记与资料，该资料目前包含 自然语言处理和推荐各领域的 面试题积累。

- [【面筋】百面百搭 群讨论收集](#面筋百面百搭-群讨论收集)
  - [推荐百面百搭](#推荐百面百搭)
    - [问题一：召回模型中，模型评价指标怎么设计？（推荐）](#问题一召回模型中模型评价指标怎么设计推荐)
    - [问题二：w&d模型的特征，哪些放到wide侧，哪些放到deep侧？ w&d模型在线上，如何做到根据实时数据更新模型（推荐）](#问题二wd模型的特征哪些放到wide侧哪些放到deep侧-wd模型在线上如何做到根据实时数据更新模型推荐)


## [推荐百面百搭](推荐百面百搭.md)

### [问题一：召回模型中，模型评价指标怎么设计？（推荐）](推荐百面百搭.md#问题一召回模型中模型评价指标怎么设计推荐)

### [问题二：w&d模型的特征，哪些放到wide侧，哪些放到deep侧？ w&d模型在线上，如何做到根据实时数据更新模型（推荐）](推荐百面百搭.md#问题二wd模型的特征哪些放到wide侧哪些放到deep侧-wd模型在线上如何做到根据实时数据更新模型推荐)

> 精彩讨论
```s
刘乙己??:
w&d模型在线上，如何做到根据实时数据更新模型
(推荐)?
模型服务器每次收到（app召回集合+当前用户特征），模型服务器返回每个app的score。
score就是对于wide & deep模型的一次 forward pass。
为了缩短响应时间10ms以内，并行预测，例如可以将分布式多线程的计算不同批次app的得分。

拒绝焦虑李某人:
https://zhuanlan.zhihu.com/p/75597761

拒绝焦虑李某人:
我看教程说，w&d里边wide的部分的优化器是用的FTRL做在线学习，利用少量样本对参数更新做类似截断的方法，满足某个条件跟新，不满足不更新，deep侧用的adagrad用全量数据做训练，我贴这个问题是想问，线上的w&d，wide可以做在线学习，那deep侧就不用了么

拒绝焦虑李某人:
王吉吉老师的一个学习笔记里写，全量数据用SGD adagrad等传统优化器来更新参数，增量数据做在线学习的时候用FTRL等，但是w&d里只有wide侧的优化器是FTRL啊

大雨:
sgd adagrad也可以增量更新啊

拒绝焦虑李某人:
增量数据带来的梯度有可能会是不正确的方向

拒绝焦虑李某人:
就是类似脏数据的意思

拒绝焦虑李某人:
如果用sgd或者adagrad没传一次增量数据就更新，模型不会跑偏么

大雨:
实时性和全局准确性是矛盾的

大雨:
可以2个版本我猜，和老版本pk一下，新的把老的赢了才用新的

拒绝焦虑李某人:
我看了下wide侧用FTRL+L1正则化是为了让模型参数更加稀疏，更在线学习没关系至于wide&deep怎么做在线学习参数实时更新，这个问题好像挺少见

刘乙己??:
我认为这个更新还是根据wide&deep这个的数据类型进行判断的叭
wide是原始特征，一些固定的数据，他不是经常变动；deep是数值特征，类别特征，是要求实时性

刘乙己??:
如果一些固定数据的变动的话，还是要进行更新的叭

刘乙己??:
更新频率的不同，导致相对来说，wide是不进行更新的

大雨:
所以要实时都实时要不实时都不实时，没有分开处理一说

拒绝焦虑李某人:
是的

大雨:
我觉得不是这个，但我说不来是哪个

_Rulcy:
我也看了一下，解释还是和业务有点关系的。论文里wide侧数据维度还是挺高的，所以需要控制Wide侧稀疏程度保证在线模型实时性。这里在线学习和模型稀疏还是紧密相关的，是因为在线学习引入的单个样本更新更可能影响到模型全局的权重，使模型丧失稀疏性，所以采用了FTRL进行更新。至于为什么只考虑Wide不考虑Deep侧实时性，是因为模型输入时Deep通常输入Embedding，相对Wide的LR模型输入One-hot那种，维度低了很多，所以Deep侧更看重精度，而对于实时性没有过多考虑。

_Rulcy:

_Rulcy:
原答案在这里，根本原因还是稀疏性和在线模型复杂度是紧密相关的

大雨:
为什么deep侧不考虑稀疏性，也会有很多id特征啊

_Rulcy:
原因是模型上线后，在wide输入是稀疏向量，而deep输入是embedding向量。wide侧稀疏的话，线上模型对wide做的压缩提升是很大的。

_Rulcy:
单独提出deep侧来看，如果使特征输入层到下一隐层稀疏的话，丧失了本来的目的，就是本来Deep就是考虑用几近全量的特征以提高模型泛化能力的，没必要再做一次特征筛选。如果使模型整体稀疏的话，同样可以在设置模型参数时就让Deep侧更轻量，不需要设置一个大的网络再通过稀疏化来缩减模型。

大雨:
可以让deep侧的时候sparse层更稀疏啊

_Rulcy:
可以但是没必要啊，拖慢网络速度的主要就是输入到embedding，Deep侧模型上线后直接采用embedding向量作为输入了，网络已经优化到一定程度了

_Rulcy:
就举例子来说，经过稀疏后，wide侧效率从100ms->10ms。经过同样的稀疏，deep侧效率可以从10ms->1ms，优化程度还是有很大区别的

大雨:
增加稀疏性还能提速?

_Rulcy:
是指线下训练增加稀疏性，线上模型就可以通过压缩提速了

大雨:
我的理解，压缩是通过降低网络开销提速的?

_Rulcy:
是这样的，简单来说一个LR模型，通过稀疏使它一半的权重为0，那么模型上限时就可以把这些权重去掉，提高效率

大雨:
所以大头还是省了网络通信开销，cpu/gpu计算耗时基本不变

_Rulcy:
emmm倒不是这个意思。这里理解两点就很好懂了：1.线下训练的模型和线上服务的模型有很大的差别，所有最终的目的都是为线上模型服务的。2.线下模型中，Deep侧训练开销更大。但是模型上线后，Wide的可能反而压力是更大的。

_Rulcy:
提高稀疏性对于线下的模型肯定是计算耗时基本不变的，但是对于线上模型一定是有提升的。

大雨:
wide端的压力更多来自存储?

_Rulcy:
wide侧的压力来自于LR没有embedding层

大雨:
忽略存储因素，忽略网络通信因素，稀疏与否并不影响，我觉得

大雨:
没有embedding层照样可以把向量变成标量，存redis

_Rulcy:
类别特征存储并不是难事啊，只是在预测时，没有Emb层的输入是稀疏的，有Emb层输入是稠密的

_Rulcy:
这种稀疏向量才是影响网络计算最大的问题

大雨:
没有问题embedding层的输入也可以是id啊，进行查表

大雨:
只不过查的不是向量，是权重，是标量

_Rulcy:
是可以的，但是主要还是one-hot嘛。类别型特征如果转化为标量，效果肯定是比不上one-hot的

_Rulcy:
如果你让LR的输入层都转为标量型，那肯定没有上面的这些顾虑了。但是为了效果，LR层的输入都是One-Hot的

_Rulcy:
类别特征转化为标量最大的问题还是，标量之间的差很难解释其意义，但又是一个重要性质

_Rulcy:
所以可以采用标量存储，但是预测时不推荐

拒绝焦虑李某人:
我觉得模型计算时间跟特征维度有关，跟网络深度也有关，跟有网络是否稀疏无关

大雨:
我不是直接转成标量，而是将id和对应的权重k,v存储

_Rulcy:
你忽视的点是，线下模型是稀疏的，线上模型会剔除稀疏权重。

拒绝焦虑李某人:
也就是参数为0的直接不算了么

_Rulcy:
对的

_Rulcy:
网络压缩也是一个热门研究方向的

大雨:
跟实现有关

大雨:
我觉得网络通信传输是大头

拒绝焦虑李某人:
deep侧不在线学习的原因是它没法稀疏化是么

大雨:
wide侧的职责我觉得是快速地捕捉变化

_Rulcy:
是稀疏化取得的提升不是太大

_Rulcy:
下班了还是给大伙推荐点好文章吧，一个很形象的将梯度下降方法的文章。http://www.360doc.com/content/20/0611/01/32196507_917690415.shtml

_Rulcy:
感觉理解更深了哈哈哈

大雨:
deep侧因为比较深，学得慢，所以要厚积薄发，举一反三

大雨:
dnn的稀疏化本来就很难
```

